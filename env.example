# Ollama Konfiguration für lokale KI
# Kopiere diese Datei nach .env:
#   cp env.example .env
#
# Ollama URL - Standard ist localhost:11434
# In Docker wird automatisch host.docker.internal verwendet
VITE_OLLAMA_URL=http://localhost:11434

# Ollama Modell - muss lokal verfügbar sein (ollama list)
VITE_OLLAMA_MODEL=gpt-oss:120b

